{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "reasonable-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "treated-wesley",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv('/home/anu/Downloads/sms_spam_ham_collection',sep='\\t',header=None)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dietary-penny",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 768 candidates, totalling 2304 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anu/yes/lib/python3.8/site-packages/sklearn/model_selection/_search.py:918: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.86767169 0.86767169 0.86767169 0.86815028 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86791098\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86982532\n",
      " 0.86767169 0.86767169 0.86767169 0.86886815 0.86767169 0.86767169\n",
      " 0.86767169 0.86886815 0.86767169 0.86767169 0.86767169 0.86815028\n",
      " 0.86767169 0.86767169 0.86767169 0.86886815 0.86767169 0.86767169\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86886815\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86982532 0.86767169 0.86767169 0.86767169 0.86886815\n",
      " 0.86767169 0.86767169 0.86767169 0.86886815 0.86767169 0.86767169\n",
      " 0.86767169 0.86815028 0.86767169 0.86767169 0.86767169 0.86886815\n",
      " 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169 0.86767169\n",
      " 0.86767169 0.86886815 0.86767169 0.86767169 0.86767169 0.86767169\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.86767169 0.86791098 0.96506341 0.96745633 0.86791098 0.86791098\n",
      " 0.96697775 0.96984925 0.86767169 0.86767169 0.96147404 0.96625987\n",
      " 0.86767169 0.86767169 0.96458483 0.9681742  0.86767169 0.86767169\n",
      " 0.96027758 0.96625987 0.86767169 0.86767169 0.95979899 0.96625987\n",
      " 0.86767169 0.86767169 0.96027758 0.96625987 0.86767169 0.86767169\n",
      " 0.94544149 0.9626705  0.86767169 0.86767169 0.96338837 0.96649916\n",
      " 0.86791098 0.86791098 0.96673845 0.9681742  0.86767169 0.86767169\n",
      " 0.96171333 0.96578129 0.86767169 0.86767169 0.96434554 0.96769562\n",
      " 0.86767169 0.86767169 0.96027758 0.965542   0.86767169 0.86767169\n",
      " 0.96027758 0.96625987 0.86767169 0.86767169 0.96027758 0.965542\n",
      " 0.86767169 0.86767169 0.94855229 0.96434554 0.86767169 0.86767169\n",
      " 0.96338837 0.96649916 0.86791098 0.86791098 0.96673845 0.9681742\n",
      " 0.86767169 0.86767169 0.96171333 0.96578129 0.86767169 0.86767169\n",
      " 0.96434554 0.96769562 0.86767169 0.86767169 0.96027758 0.965542\n",
      " 0.86767169 0.86767169 0.96027758 0.96625987 0.86767169 0.86767169\n",
      " 0.96027758 0.965542   0.86767169 0.86767169 0.94855229 0.96434554\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.95549174 0.95453458 0.98109596 0.98133525 0.9480737  0.95142379\n",
      " 0.98301029 0.98205312 0.94950945 0.95166308 0.98181383 0.98085666\n",
      " 0.92701603 0.94113424 0.98324958 0.98157454 0.94639866 0.95142379\n",
      " 0.98157454 0.98085666 0.90428332 0.92940895 0.982771   0.98229241\n",
      " 0.94639866 0.95142379 0.98157454 0.98085666 0.87437186 0.89877961\n",
      " 0.98181383 0.98013879 0.95333812 0.95190237 0.98061737 0.98061737\n",
      " 0.94663795 0.94759512 0.98396746 0.98157454 0.94711654 0.94759512\n",
      " 0.98181383 0.9798995  0.92677674 0.93634841 0.98324958 0.98205312\n",
      " 0.94496291 0.94759512 0.98133525 0.97966021 0.9018904  0.92438382\n",
      " 0.98348887 0.98229241 0.94496291 0.94759512 0.98133525 0.97966021\n",
      " 0.87485044 0.89734386 0.98181383 0.98157454 0.95333812 0.95190237\n",
      " 0.98061737 0.98061737 0.94663795 0.94759512 0.98396746 0.98157454\n",
      " 0.94711654 0.94759512 0.98181383 0.9798995  0.92677674 0.93634841\n",
      " 0.98324958 0.98205312 0.94496291 0.94759512 0.98133525 0.97966021\n",
      " 0.9018904  0.92438382 0.98348887 0.98229241 0.94496291 0.94759512\n",
      " 0.98133525 0.97966021 0.87485044 0.89734386 0.98181383 0.98157454]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3,\n",
       "             estimator=Pipeline(steps=[('vect',\n",
       "                                        TfidfVectorizer(stop_words='english')),\n",
       "                                       ('est', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'est__C': (0.01, 0.1, 1, 10),\n",
       "                         'est__penalty': ('l1', 'l2'),\n",
       "                         'vect__max_df': (0.25, 0.5, 0.75),\n",
       "                         'vect__max_features': (2500, 5000, 10000, None),\n",
       "                         'vect__ngram_range': ((1, 1), (1, 2)),\n",
       "                         'vect__norm': ('l1', 'l2'), 'vect__stop_words': [None],\n",
       "                         'vect__use_idf': (True, False)},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lb=LabelBinarizer()\n",
    "pipe = Pipeline([('vect',TfidfVectorizer(stop_words='english')),('est',LogisticRegression())])\n",
    "parameters={\n",
    "    'vect__max_df':(0.25,0.5,0.75),\n",
    "    'vect__stop_words':([None]),\n",
    "    'vect__max_features':(2500,5000,10000,None),\n",
    "    'vect__ngram_range':((1,1),(1,2)),\n",
    "    'vect__use_idf':(True,False),\n",
    "    'vect__norm':('l1','l2'),\n",
    "    'est__penalty':('l1','l2'),\n",
    "    'est__C':(0.01,0.1,1,10)\n",
    "}\n",
    "# est_c => lambda value\n",
    "#vect__max_features=>(2500,5000,10000,None)#can take either 1st 2500 values,or 5000 or none\n",
    "grid_search=GridSearchCV(pipe,parameters,n_jobs=-1,\\\n",
    "                        scoring='accuracy',cv=3,verbose=1)\n",
    "xtrain,xtest,ytrain,ytest=train_test_split(data.iloc[:,1],data.iloc[:,0])\n",
    "ytrain=np.array([num[0] for num in lb.fit_transform(ytrain)])\n",
    "grid_search.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dedicated-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:0.983967\n"
     ]
    }
   ],
   "source": [
    "print('Best score:%2f'%grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "entire-niagara",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'est__C': 10, 'est__penalty': 'l2', 'vect__max_df': 0.5, 'vect__max_features': 2500, 'vect__ngram_range': (1, 2), 'vect__norm': 'l2', 'vect__stop_words': None, 'vect__use_idf': True}\n"
     ]
    }
   ],
   "source": [
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "gothic-leader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "est__C: 10\n",
      "est__penalty: 'l2'\n",
      "vect__max_df: 0.5\n",
      "vect__max_features: 2500\n",
      "vect__ngram_range: (1, 2)\n",
      "vect__norm: 'l2'\n",
      "vect__stop_words: None\n",
      "vect__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "best_parameters=grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('%s: %r'%(param_name,best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "vanilla-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test=np.array([num[0] for num in lb.fit_transform(ytest)])\n",
    "predictions=grid_search.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "recovered-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9834888729361091\n",
      "Precision score: 0.9776536312849162\n",
      "Recall score: 0.9020618556701031\n",
      "F1 score: 0.938337801608579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score\n",
    "\n",
    "print('Accuracy score:',accuracy_score(y_test,predictions))\n",
    "print('Precision score:',precision_score(y_test,predictions))\n",
    "print('Recall score:',recall_score(y_test,predictions))\n",
    "print('F1 score:',f1_score(y_test,predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "speaking-pursuit",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
